So, is it meaningful to convert dates and numbers to strings?

Short Answer: Yes — for scoring metrics like BLEU, ROUGE, and METEOR, it’s necessary.

These metrics work purely on tokens — they don’t understand the meaning of a number like 25 or a date like 2023-04-21. To them, it’s just a token. So you have two choices:

⸻

Why convert?
	•	These metrics require uniform string inputs.
	•	If you don’t convert numbers or dates to strings, you’ll get errors (int has no attribute 'len', etc.).
	•	Once converted, it treats “25” just like any other word, which is okay because that’s how BLEU/ROUGE work.

⸻

But does it affect accuracy or fairness of the score?

Yes, potentially — context matters.

Example:
	•	Reference: “The event is on 2023-04-20.”
	•	Hypothesis: “The event is on 2023-04-21.”

Both will tokenize and match most tokens, but the date is different, so BLEU may still give a good score even though the actual meaning differs.

So in critical cases (like factual QA, summaries, medical data), BLEU/ROUGE/METEOR might miss important semantic differences.

⸻

How to handle this better?

If dates and numbers are semantically important (e.g., summaries, reports, medical records), then you might want to:
	1.	Use semantic similarity metrics like:
	•	BERTScore
	•	BLEURT
	•	GPT-based evals (e.g., LLM-as-a-judge)
	2.	Or manually check date/number accuracy alongside BLEU/ROUGE.

⸻

TL;DR:
	•	Yes, you should convert dates and numbers to strings to avoid errors.
	•	But remember: BLEU/ROUGE/METEOR aren’t always good at catching factual mismatches, so consider using semantic or factual evaluation tools too if that matters in your task.

Want help setting up BERTScore or a factual consistency checker?

----

Got it — since your sample answers have important numerical entries (like dates, ages, etc.) and you want meaningful evaluation, here’s a better way to evaluate your text outputs beyond just using BLEU/ROUGE/METEOR.

⸻

Step-by-Step Guide to Evaluation

Step 1: Still Convert Dates/Numbers to Strings

First, ensure BLEU/ROUGE/METEOR don’t break:

def normalize_tokens(sentence):
    return [
        token.strftime("%Y-%m-%d") if hasattr(token, "strftime") else str(token)
        for token in sentence
    ]

Step 2: Use BLEU, ROUGE, METEOR (for general fluency/structure)

These metrics are still useful for fluency, grammar, and partial overlap — just take them with a grain of salt when numbers change.

⸻

Step 3: Add Semantic Evaluation

A. Use BERTScore (semantic token-level similarity)

pip install bert-score

from bert_score import score

references = ["The event is on 2023-04-20."]
candidates = ["The event is on 2023-04-21."]

P, R, F1 = score(candidates, references, lang="en", verbose=True)
print(f"Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1: {F1.item():.4f}")

Why BERTScore?
	•	It checks meaning similarity, not just exact word match.
	•	If the sentence is almost the same but just a date changed, it will reflect some drop, but not as harsh as BLEU.

⸻

B. Factual Consistency Checks (Optional)

You can go next level with tools like:
	•	LLM-as-a-judge (use GPT-style model to compare reference and hypothesis and rate factual correctness)
	•	BLEURT (Google’s model for human-aligned scoring)

Want code for these?

⸻

Step 4: Custom Accuracy for Dates/Numbers (Optional)

If specific values like dates or numbers are critical, write a custom checker:

import re

def extract_dates(text):
    return re.findall(r"\d{4}-\d{2}-\d{2}", text)

ref = "The event is on 2023-04-20."
hyp = "The event is on 2023-04-21."

ref_dates = extract_dates(ref)
hyp_dates = extract_dates(hyp)

print("Dates match:", ref_dates == hyp_dates)

You can extend this to ages, amounts, etc.

⸻

TL;DR:

Metric	Evaluates	Good For
BLEU / ROUGE	Token/phrase overlap	Fluency, rough correctness
METEOR	Soft match + synonyms	Fluency, paraphrasing
BERTScore	Semantic similarity	Paraphrasing, meaning matching
Custom checks	Dates/numbers explicitly	Factual accuracy
LLM-as-judge / BLEURT	Overall coherence + facts	Human-like judgment



⸻
